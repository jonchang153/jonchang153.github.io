---
layout: post
title: Web Scraping with Scrapy
---

In this blog post, I'll explain how to scrape information off a website with Scrapy. In particular, I will be demonstrating how to retreive data of all the movies or TV shows in which actors of a particular movie have played in, given only the IMDB page of the movie. In this example, I'll be using one of my favorite movies, "The Shawshank Redemption", which has the following IMDB page: https://www.imdb.com/title/tt0111161/

## Setup the project

To create the project, open a terminal in the desired directory, and switch to an environment that has Scrapy installed. Then, run the following command.
```
scrapy startproject IMDB_scraper
```
This will create a project folder with many default files. The actual scraper will be a Python file in the spiders folder. Create a file `imdb_spider.py` in the `spiders` folder inside the `IMDB_scraper` folder. Then, add the following code to initialize the spider:
```python
import scrapy

# create a class that inherits from Spider
class ImdbSpider(scrapy.Spider):
  name = 'imdb_spider' # give this spider a name that can be called from the terminal
    
  # paste the url of the IMDB page of the movie to be scraped
  start_urls = ['https://www.imdb.com/title/tt0111161/']
```

## Adding scraping methods
We will be implementing three methods that will achieve the desired behavior. This will be making use of css selectors; you can follow along by inspecting the page of your movie and analyzing the HTML of the page.
```python
def parse(self, response):
  """
  Accesses the full credits (including the full cast) of the specified IMDB page
  and passes this to the parse_full_credits() method.
  """

  # use urljoin(), which appends the argument to the current url stored in the 
  # response object, to access the full credits of the movie's main IMDB page
  next_page = response.urljoin('fullcredits/')

  # use scrapy.Request to yield this new URL while calling parse_full_credits()
  yield scrapy.Request(next_page, callback = self.parse_full_credits)
```
The first method, `parse()`, is called when the scraper is accessed through the terminal. This begins the scraping process by calling the next method while redirecting the response object's page to the full credits page of "The Shawshank Redemption".


```python
def parse_full_credits(self, response):
  """
  Uses css selectors to get the link to each actor's page found in the full credits
  page of the movie. Then passes each actor's page to the parse_actor_page() method.
  """

  # this css gets the href attribute of all a tags that have a td ancestor with 
  # class primary_photo, which gets the url of each actor listed in the full 
  # credits page
  actor_pages = response.css('td.primary_photo a::attr(href)').getall()

  # loop through each actor's url
  for actor_page in actor_pages:
    # use urljoin() to yield the url of this actor while calling parse_actor_page()
    yield scrapy.Request(response.urljoin(actor_page), callback = self.parse_actor_page)
```
The next method, `parse_full_credits()`, uses css selectors to get the page of each actor in the cast of "The Shawshank Redemption". We use `response.css()` to communicate with the HTML of the response object's url, which returns a sort of HTML element. To actually extract textual data from a css selection, we use `get()` or `getall()`. `get()` returns the first match, even if there are several matches. `getall()` returns a list of matches. Because we want a list of every actor on the page, we use `getall()`.

The following also works instead:
```python
actor_pages = [a.attrib["href"] for a in response.css("td.primary_photo a")]
```
In this, `attrib[]` can get the attribute of an HTML tag. It behaves like `get()` returning the first match, unless used in a list comprehension.

```python
def parse_actor_page(self, response):
  """
  Uses css selectors to get the actor name and the name of every movie and TV show
  they starred in, for a given actor's url page. Each of these movie or TV show's 
  name is yielded in a dictionary with the actor's name.
  """

  # select the text of the FIRST span tag with class itemprop with an h1 ancestor
  actor_name = response.css('h1 span.itemprop::text').get()

  # select the text of ALL a tags that are ancestors of b tags that are ancestors
  # of div tags with class containing filmo-row and id containing actor
  names = response.css('div.filmo-row[id*=actor] b a::text').getall()

  # loop through each movie or TV show name
  for movie_or_TV_name in names:
    # yield a dictionary containing the actor's name and the movie or TV show's name
    yield {
      "actor" : actor_name, 
      "movie_or_TV_name" : movie_or_TV_name
    }
```
In the third method, `parse_actor_page()`, we again use css selectors to get the name of the actor and the name of every movie or TV show that the actor played in, found on the actor page passed to this method. This yields a dictionary for each movie or TV show name of each actor, which can be stored in a .csv file when this scraper is called.


## Access the results
Use the following command in terminal to run the scraper and aggregate the results into a .csv file named `results.csv`. Make sure you are in the directory of the IMDB_scraper project.
```
scrapy crawl imdb_spider -o results.csv
```


## Conclusion

Finally, we can interpret this data in various ways. Here, I have used it for recommendations by listing other movies or TV shows that share a large amount of actors with the specified movie. The top 20 results are displayed.

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>movie</th>
      <th>number of shared actors</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>The Shawshank Redemption</td>
      <td>65.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ER</td>
      <td>11.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>CSI: Crime Scene Investigation</td>
      <td>10.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Law &amp; Order</td>
      <td>10.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>The West Wing</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>The Practice</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>NYPD Blue</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Cold Case</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>L.A. Law</td>
      <td>8.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>The Twilight Zone</td>
      <td>8.0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>24</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Law &amp; Order: Special Victims Unit</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>The Equalizer</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Law &amp; Order: Criminal Intent</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>NCIS</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Criminal Minds</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Matlock</td>
      <td>5.0</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Seinfeld</td>
      <td>5.0</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Ally McBeal</td>
      <td>5.0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Monk</td>
      <td>5.0</td>
    </tr>
  </tbody>
</table>
</div>

You can visit my GitHub project repository to see the full implementation of my web scraper: https://github.com/jonchang153/Web-Scraping-with-Scrapy